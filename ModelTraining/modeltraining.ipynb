{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/Users/khumapokharel/Desktop/deepLearning/HandgesuretoAlphabet/final_data.pkl\",'rb') as file:\n",
    "    data=pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364, 43)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "0    0\n",
       "0    0\n",
       "0    0\n",
       "0    0\n",
       "    ..\n",
       "0    8\n",
       "0    8\n",
       "0    8\n",
       "0    8\n",
       "0    8\n",
       "Name: 42, Length: 364, dtype: int64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=data.iloc[:,-1]\n",
    "y=y.astype(int)\n",
    "y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42\n",
       "0    49\n",
       "1    49\n",
       "8    48\n",
       "2    46\n",
       "7    42\n",
       "3    41\n",
       "6    39\n",
       "4    25\n",
       "5    25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=data.iloc[:,:-1]\n",
    "x=x.astype(float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting data into the train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(x,y,random_state=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(273, 42)\n",
      "(91, 42)\n",
      "(273,)\n",
      "(91,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# data[0].plot(kind='kde')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.480745</td>\n",
       "      <td>0.688575</td>\n",
       "      <td>0.464335</td>\n",
       "      <td>0.573078</td>\n",
       "      <td>0.469602</td>\n",
       "      <td>0.477354</td>\n",
       "      <td>0.473369</td>\n",
       "      <td>0.418366</td>\n",
       "      <td>0.477322</td>\n",
       "      <td>0.385976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.558879</td>\n",
       "      <td>0.555904</td>\n",
       "      <td>0.593938</td>\n",
       "      <td>0.546803</td>\n",
       "      <td>0.631406</td>\n",
       "      <td>0.490491</td>\n",
       "      <td>0.655267</td>\n",
       "      <td>0.458344</td>\n",
       "      <td>0.672970</td>\n",
       "      <td>0.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.348582</td>\n",
       "      <td>0.595860</td>\n",
       "      <td>0.438952</td>\n",
       "      <td>0.560593</td>\n",
       "      <td>0.517214</td>\n",
       "      <td>0.478639</td>\n",
       "      <td>0.579410</td>\n",
       "      <td>0.415947</td>\n",
       "      <td>0.635645</td>\n",
       "      <td>0.397735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.521425</td>\n",
       "      <td>-0.085137</td>\n",
       "      <td>0.389080</td>\n",
       "      <td>0.219523</td>\n",
       "      <td>0.396193</td>\n",
       "      <td>0.101103</td>\n",
       "      <td>0.410228</td>\n",
       "      <td>0.025191</td>\n",
       "      <td>0.430010</td>\n",
       "      <td>-0.043597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.397521</td>\n",
       "      <td>0.424296</td>\n",
       "      <td>0.428511</td>\n",
       "      <td>0.462336</td>\n",
       "      <td>0.451803</td>\n",
       "      <td>0.481919</td>\n",
       "      <td>0.437699</td>\n",
       "      <td>0.481746</td>\n",
       "      <td>0.412197</td>\n",
       "      <td>0.474106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400144</td>\n",
       "      <td>0.439577</td>\n",
       "      <td>0.402828</td>\n",
       "      <td>0.300776</td>\n",
       "      <td>0.379522</td>\n",
       "      <td>0.394306</td>\n",
       "      <td>0.374488</td>\n",
       "      <td>0.437882</td>\n",
       "      <td>0.382301</td>\n",
       "      <td>0.444049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.671389</td>\n",
       "      <td>0.442457</td>\n",
       "      <td>0.563473</td>\n",
       "      <td>0.454812</td>\n",
       "      <td>0.437697</td>\n",
       "      <td>0.473230</td>\n",
       "      <td>0.341065</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.291539</td>\n",
       "      <td>...</td>\n",
       "      <td>0.535713</td>\n",
       "      <td>0.561651</td>\n",
       "      <td>0.565412</td>\n",
       "      <td>0.553678</td>\n",
       "      <td>0.608830</td>\n",
       "      <td>0.486531</td>\n",
       "      <td>0.635861</td>\n",
       "      <td>0.445945</td>\n",
       "      <td>0.656808</td>\n",
       "      <td>0.408847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.380049</td>\n",
       "      <td>0.616673</td>\n",
       "      <td>0.417857</td>\n",
       "      <td>0.616520</td>\n",
       "      <td>0.442822</td>\n",
       "      <td>0.621905</td>\n",
       "      <td>0.444438</td>\n",
       "      <td>0.652751</td>\n",
       "      <td>0.435262</td>\n",
       "      <td>0.687145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.405029</td>\n",
       "      <td>0.630461</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.533358</td>\n",
       "      <td>0.372923</td>\n",
       "      <td>0.628600</td>\n",
       "      <td>0.377633</td>\n",
       "      <td>0.646612</td>\n",
       "      <td>0.380550</td>\n",
       "      <td>0.631008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.393380</td>\n",
       "      <td>0.388996</td>\n",
       "      <td>0.435712</td>\n",
       "      <td>0.387746</td>\n",
       "      <td>0.473389</td>\n",
       "      <td>0.336684</td>\n",
       "      <td>0.500019</td>\n",
       "      <td>0.298563</td>\n",
       "      <td>0.517650</td>\n",
       "      <td>0.259380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510612</td>\n",
       "      <td>0.223425</td>\n",
       "      <td>0.424725</td>\n",
       "      <td>0.204137</td>\n",
       "      <td>0.458493</td>\n",
       "      <td>0.152718</td>\n",
       "      <td>0.485634</td>\n",
       "      <td>0.169349</td>\n",
       "      <td>0.503076</td>\n",
       "      <td>0.205703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588367</td>\n",
       "      <td>0.369496</td>\n",
       "      <td>0.634471</td>\n",
       "      <td>0.447159</td>\n",
       "      <td>0.676932</td>\n",
       "      <td>0.521568</td>\n",
       "      <td>0.692640</td>\n",
       "      <td>0.598315</td>\n",
       "      <td>0.688744</td>\n",
       "      <td>0.647179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650655</td>\n",
       "      <td>0.500110</td>\n",
       "      <td>0.660516</td>\n",
       "      <td>0.419787</td>\n",
       "      <td>0.635256</td>\n",
       "      <td>0.537489</td>\n",
       "      <td>0.621051</td>\n",
       "      <td>0.515979</td>\n",
       "      <td>0.623817</td>\n",
       "      <td>0.481969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.346893</td>\n",
       "      <td>0.188462</td>\n",
       "      <td>0.411028</td>\n",
       "      <td>0.234771</td>\n",
       "      <td>0.462499</td>\n",
       "      <td>0.258423</td>\n",
       "      <td>0.475003</td>\n",
       "      <td>0.290622</td>\n",
       "      <td>0.447979</td>\n",
       "      <td>0.300919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.387430</td>\n",
       "      <td>0.192639</td>\n",
       "      <td>0.364966</td>\n",
       "      <td>0.086038</td>\n",
       "      <td>0.357135</td>\n",
       "      <td>0.196703</td>\n",
       "      <td>0.355279</td>\n",
       "      <td>0.213348</td>\n",
       "      <td>0.355293</td>\n",
       "      <td>0.190606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.590197</td>\n",
       "      <td>0.693687</td>\n",
       "      <td>0.605286</td>\n",
       "      <td>0.616969</td>\n",
       "      <td>0.599064</td>\n",
       "      <td>0.545397</td>\n",
       "      <td>0.580515</td>\n",
       "      <td>0.498749</td>\n",
       "      <td>0.564192</td>\n",
       "      <td>0.466904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485760</td>\n",
       "      <td>0.370816</td>\n",
       "      <td>0.518847</td>\n",
       "      <td>0.563170</td>\n",
       "      <td>0.501171</td>\n",
       "      <td>0.491970</td>\n",
       "      <td>0.489104</td>\n",
       "      <td>0.453978</td>\n",
       "      <td>0.478233</td>\n",
       "      <td>0.424068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.369060</td>\n",
       "      <td>0.373242</td>\n",
       "      <td>0.424701</td>\n",
       "      <td>0.345090</td>\n",
       "      <td>0.457045</td>\n",
       "      <td>0.294247</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.251950</td>\n",
       "      <td>0.498679</td>\n",
       "      <td>0.201931</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480996</td>\n",
       "      <td>0.193317</td>\n",
       "      <td>0.394971</td>\n",
       "      <td>0.168103</td>\n",
       "      <td>0.428978</td>\n",
       "      <td>0.099424</td>\n",
       "      <td>0.458816</td>\n",
       "      <td>0.116350</td>\n",
       "      <td>0.477193</td>\n",
       "      <td>0.158471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>273 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.480745  0.688575  0.464335  0.573078  0.469602  0.477354  0.473369   \n",
       "0   0.348582  0.595860  0.438952  0.560593  0.517214  0.478639  0.579410   \n",
       "0   0.397521  0.424296  0.428511  0.462336  0.451803  0.481919  0.437699   \n",
       "0   0.453457  0.671389  0.442457  0.563473  0.454812  0.437697  0.473230   \n",
       "0   0.380049  0.616673  0.417857  0.616520  0.442822  0.621905  0.444438   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "0   0.393380  0.388996  0.435712  0.387746  0.473389  0.336684  0.500019   \n",
       "0   0.588367  0.369496  0.634471  0.447159  0.676932  0.521568  0.692640   \n",
       "0   0.346893  0.188462  0.411028  0.234771  0.462499  0.258423  0.475003   \n",
       "0   0.590197  0.693687  0.605286  0.616969  0.599064  0.545397  0.580515   \n",
       "0   0.369060  0.373242  0.424701  0.345090  0.457045  0.294247  0.483803   \n",
       "\n",
       "          7         8         9   ...        32        33        34        35  \\\n",
       "0   0.418366  0.477322  0.385976  ...  0.558879  0.555904  0.593938  0.546803   \n",
       "0   0.415947  0.635645  0.397735  ...  0.521425 -0.085137  0.389080  0.219523   \n",
       "0   0.481746  0.412197  0.474106  ...  0.400144  0.439577  0.402828  0.300776   \n",
       "0   0.341065  0.493902  0.291539  ...  0.535713  0.561651  0.565412  0.553678   \n",
       "0   0.652751  0.435262  0.687145  ...  0.405029  0.630461  0.365000  0.533358   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "0   0.298563  0.517650  0.259380  ...  0.510612  0.223425  0.424725  0.204137   \n",
       "0   0.598315  0.688744  0.647179  ...  0.650655  0.500110  0.660516  0.419787   \n",
       "0   0.290622  0.447979  0.300919  ...  0.387430  0.192639  0.364966  0.086038   \n",
       "0   0.498749  0.564192  0.466904  ...  0.485760  0.370816  0.518847  0.563170   \n",
       "0   0.251950  0.498679  0.201931  ...  0.480996  0.193317  0.394971  0.168103   \n",
       "\n",
       "          36        37        38        39        40        41  \n",
       "0   0.631406  0.490491  0.655267  0.458344  0.672970  0.424700  \n",
       "0   0.396193  0.101103  0.410228  0.025191  0.430010 -0.043597  \n",
       "0   0.379522  0.394306  0.374488  0.437882  0.382301  0.444049  \n",
       "0   0.608830  0.486531  0.635861  0.445945  0.656808  0.408847  \n",
       "0   0.372923  0.628600  0.377633  0.646612  0.380550  0.631008  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "0   0.458493  0.152718  0.485634  0.169349  0.503076  0.205703  \n",
       "0   0.635256  0.537489  0.621051  0.515979  0.623817  0.481969  \n",
       "0   0.357135  0.196703  0.355279  0.213348  0.355293  0.190606  \n",
       "0   0.501171  0.491970  0.489104  0.453978  0.478233  0.424068  \n",
       "0   0.428978  0.099424  0.458816  0.116350  0.477193  0.158471  \n",
       "\n",
       "[273 rows x 42 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42\n",
       "0    38\n",
       "2    37\n",
       "8    36\n",
       "6    34\n",
       "1    33\n",
       "3    30\n",
       "7    29\n",
       "5    21\n",
       "4    15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42\n",
       "1    16\n",
       "7    13\n",
       "8    12\n",
       "0    11\n",
       "3    11\n",
       "4    10\n",
       "2     9\n",
       "6     5\n",
       "5     4\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=50)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, max_depth=50)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', max_depth=50)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt=DecisionTreeClassifier(max_depth=50,criterion='entropy')\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=dt.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8681318681318682"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "accuracy_score(y_train,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khumapokharel/anaconda3/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-12 {color: black;}#sk-container-id-12 pre{padding: 0;}#sk-container-id-12 div.sk-toggleable {background-color: white;}#sk-container-id-12 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-12 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-12 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-12 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-12 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-12 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-12 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-12 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-12 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-12 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-12 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-12 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-12 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-12 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-12 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-12 div.sk-item {position: relative;z-index: 1;}#sk-container-id-12 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-12 div.sk-item::before, #sk-container-id-12 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-12 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-12 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-12 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-12 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-12 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-12 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-12 div.sk-label-container {text-align: center;}#sk-container-id-12 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-12 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-12\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" checked><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(multi_class=&#x27;multinomial&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(multi_class='multinomial')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred=logreg_classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.945054945054945"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train,train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred=logreg_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9120879120879121"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# saving the model\n",
    "#===============================\n",
    "pickle_file_path = \"model.pkl\"\n",
    "\n",
    "with open(pickle_file_path, 'wb') as pickle_file:\n",
    "    pickle.dump(logreg_classifier, pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
